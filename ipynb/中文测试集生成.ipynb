{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m105 packages\u001b[0m \u001b[2min 6ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m99 packages\u001b[0m \u001b[2min 0.09ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add ragas langchain-openai langchain-community pandas ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 下载和加载中文语料库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 准备中文数据集（比如中国四大名著）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/tennessine/corpus.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[0m\n",
      "├── \u001b[01;34mcorpus\u001b[0m\n",
      "│   ├── \u001b[00mLLM术语表.md\u001b[0m\n",
      "│   ├── \u001b[01;32m三国演义.txt\u001b[0m\n",
      "│   ├── \u001b[01;32m水浒传.txt\u001b[0m\n",
      "│   ├── \u001b[01;32m红楼梦.txt\u001b[0m\n",
      "│   └── \u001b[01;32m西游记.txt\u001b[0m\n",
      "└── \u001b[00m中文测试集生成.ipynb\u001b[0m\n",
      "\n",
      "2 directories, 6 files\n"
     ]
    }
   ],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 使用LangChain加载中文文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "# 指向包含中文文本的目录\n",
    "path = \"corpus/\"\n",
    "loader = DirectoryLoader(path, glob=\"**/*.md\", loader_cls=TextLoader)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "0",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "1",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "2",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "3",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "23b2fd07-db14-42a8-b487-6bbb4740dd43",
       "rows": [
        [
         "0",
         "('id', None)",
         "('metadata', {'source': 'corpus/LLM术语表.md'})",
         "('page_content', 'A\\n智能代理（Agent）\\n智能代理是一种自主 AI 系统，能够根据环境信息做出决策并执行任务。在 Dify 平台中，智能代理结合大语言模型的理解能力与外部工具的交互能力，可以自动完成从简单到复杂的一系列操作，如搜索信息、调用 API 或生成内容。\\n智能体工作流（Agentic Workflow）\\n智能体工作流是一种任务编排方法，允许 AI 系统通过多个步骤自主解决复杂问题。例如，一个智能体工作流可以先理解用户问题，然后查询知识库，接着调用计算工具，最后整合信息生成完整回答，全程无需人工干预。\\n自动语音识别（ASR, Automatic Speech Recognition）\\n自动语音识别技术将人类语音转换为文本，是语音交互应用的基础。这项技术使用户可以通过说话而非打字与 AI 系统交互，广泛应用于语音助手、会议记录和无障碍服务等场景。\\nB\\n思维骨架（BoT, Backbone of Thought）\\n思维骨架是一种结构化思考框架，为大语言模型提供推理的主干结构。它帮助模型在处理复杂问题时保持清晰的思考路径，类似于论文的提纲或决策树的骨架。\\nC\\n对话流（Chatflow）\\n对话流是一种面向对话场景的工作流编排模式，专为需要多步逻辑处理的交互式应用设计。与普通工作流相比，对话流额外支持对话历史记忆、问题理解和上下文管理等功能，使大模型能够在多轮对话中保持连贯性。\\n分段（Chunking）\\n分段是将长文本拆分成较小内容块的处理技术，使检索系统能更精准地找到相关信息。合理的分段策略既要考虑内容的语义完整性，也要满足语言模型的上下文窗口限制，从而提高检索和生成质量。\\n引用与归属（Citation and Attribution）\\n引用与归属功能让 AI 系统能够清晰标明信息来源，提高响应的可信度和透明度。当系统基于知识库内容生成回答时，可以自动标注引用的文档名称、页码或 URL，让用户了解信息的出处。\\n思维链（CoT, Chain of Thought）\\n思维链是一种提示技术，引导大语言模型展示其逐步思考过程。例如，解决数学问题时，模型会先列出已知条件，然后按照推理步骤一步步求解，最后得出结论，整个过程类似人类的思考方式。\\nD\\n领域特定语言（DSL, Domain-Specific Language）\\n领域特定语言是为特定应用领域设计的编程语言或配置格式。Dify DSL 是一种基于 YAML 格式的应用工程文件标准，用于定义 AI 应用的各项配置，包括模型参数、提示词设计和工作流编排，使非专业开发者也能构建复杂 AI 应。\\nE\\n提取、转换、加载（ETL, Extract, Transform, Load）\\nETL 是数据处理的经典流程：提取原始数据，转换为适合分析的格式，然后加载到目标系统。在 AI 文档处理中，ETL 可能包括从 PDF 提取文本、清理格式、分割内容、计算嵌入向量，最后加载到向量数据库中，为 RAG 系统做准备。\\nF\\n全文检索（Full-text Search）\\n全文检索是索引文档中的所有词汇，从而允许用户查询任意词汇，并返回包含这些词汇的文本片段。这种技术是现代搜索引擎的基础，能够扫描整个文档集合，无论内容规模多大，都能快速定位包含特定字词或短语的内容。全文检索通常结合倒排索引等数据结构实现高效查询，适用于各类文档库、知识库和搜索系统。\\n频率惩罚（Frequency Penalty）\\n频率惩罚是一种文本生成控制参数，通过降低频繁出现词汇的生成概率来增加输出的多样性。值越高，模型越倾向于使用多样化的词汇和表达方式；值为 0 时，模型不会特意避免重复使用相同词汇。\\n函数调用（Function Calling）\\n函数调用是大型语言模型的能力，允许模型识别何时需要调用特定函数并提供所需参数。例如，当用户询问天气时，模型可以自动调用天气 API，构造正确的参数格式（城市、日期），然后根据 API 返回结果生成回答。\\nG\\n通用分段模式（General Chunking Pattern）\\n通用分段模式是一种简单的文本分割策略，将文档拆分为相互独立的内容块。这种模式适合结构清晰、段落相对独立的文档，如产品说明书或百科条目，每个分段可以独立理解而不严重依赖上下文。\\n思维图（GoT, Graph of Thought）\\n思维图是一种将思考过程表示为网络结构的方法，捕捉概念之间的复杂关系。不同于线性的思维链，思维图可以表达分支、循环和多路径的思考模式，适合处理有多个相互关联因素的复杂问题。\\nH\\n混合检索（Hybrid Search）\\n混合检索结合关键词匹配和语义搜索的优势，提供更全面的检索结果。例如，当搜索\"苹果营养成分\"时，混合检索既能找到包含\"苹果\"和\"营养\"关键词的文档，也能找到讨论\"水果健康价值\"等相关语义的内容，通过权重调整或重排序选出最优结果。\\nI\\n倒排索引（Inverted Index）\\n倒排索引是搜索引擎的核心数据结构，它记录每个词出现在哪些文档中。与传统索引从文档找内容不同，倒排索引从词汇出发找文档，大幅提高全文检索速度。例如，\"人工智能\"一词的索引项会列出所有包含这个词的文档 ID 和位置。\\nK\\n关键词检索（Keyword Search）\\n关键词检索是基于精确匹配的搜索方法，查找包含特定词汇的文档。这种方法计算效率高，适合用户明确知道要查找的术语的场景，如产品型号、专有名词或特定命令，但可能会漏掉使用同义词或相关概念表达的内容。\\n知识库（Knowledge Base）\\n知识库是 AI 应用中存储结构化信息的数据库，为模型提供专业知识来源。在 Dify 平台中，知识库可以包含各种文档（PDF、Word、网页等），经过处理后供 AI 检索并用于生成准确、有根据的回答，特别适合构建领域专家型应用。\\n知识检索（Knowledge Retrieval）\\n知识检索是从知识库中找出与用户问题最相关信息的过程，是 RAG 系统的关键环节。有效的知识检索不仅要找到相关内容，还要控制返回的信息量，避免无关内容干扰模型，同时提供足够背景确保回答准确完整。\\nL\\n大型语言模型（LLM, Large Language Model）\\n大型语言模型是通过海量文本训练的 AI 模型，能够理解和生成人类语言。现代 LLM（如 GPT 系列、Claude 等）可以撰写文章、回答问题、编写代码，甚至进行推理，它们是各种 AI 应用的核心引擎，尤其适合需要语言理解和生成的场景。\\n本地模型推理（Local Model Inference）\\n本地模型推理是在用户自己的设备上运行 AI 模型的过程，而非依赖云服务。这种方式提供更好的隐私保护（数据不离开本地）和更低的延迟（无需网络传输），适合处理敏感数据或需要离线工作的场景，但通常受限于本地设备的计算能力。\\nM\\n模型即服务（MaaS, Model-as-a-Service）\\n模型即服务是一种云服务模式，提供商通过 API 提供预训练模型的访问。用户无需关心模型的训练、部署和维护，只需调用 API 并支付使用费用，大幅降低了 AI 应用的开发门槛和基础设施成本，适合快速验证想法或构建原型。\\n最大标记数（Max Tokens）\\n最大标记数控制模型在单次响应中生成的最大字符量。一个标记大约相当于 4 个字符或 3/4 个英文单词。设置合理的最大标记数可以控制回答的长度，避免过于冗长的输出，同时确保完整表达必要信息。例如，一篇简短摘要可能设为 200 标记，而详细报告可能需要 2000 标记。\\n记忆（Memory）\\n记忆是 AI 系统保存和使用历史交互信息的能力，使多轮对话保持连贯。有效的记忆机制让 AI 能够理解上下文引用、记住用户偏好、追踪长期目标，从而提供个性化且有连续性的用户体验，避免重复询问已提供的信息。\\n元数据（Metadata）\\n元数据是描述数据的数据，提供关于内容的结构化信息，如文档的创建时间、作者、标题、标签、文件格式等属性信息等。这些信息可用于组织、分类和检索内容。通过元数据，系统可以实现更精确的内容管理和检索，例如按时间范围筛选文档、按作者分类或根据文件类型过滤搜索结果。\\n元数据筛选（Metadata Filtering）\\n元数据筛选利用文档属性信息（如标题、作者、日期、分类标签）进行内容过滤。例如，用户可以限定只检索特定日期范围内的技术文档，或只查询特定部门的报告，从而在检索前缩小范围，提高查找效率和结果相关性。\\n多模态模型（Multimodal Model）\\n多模态模型能处理多种类型的输入数据，如文本、图像、音频等。这类模型打破了传统 AI 的单一感知限制，可以理解图片内容、分析视频场景、识别声音情绪，为更全面的信息理解创造可能，适用于需要跨媒体理解的复杂应用场景。\\n多工调用（Multi-tool-call）\\n多工具调用是模型在单次响应中调用多个不同工具的能力。例如，处理\"比较北京和上海明天的天气并推荐适合的衣着\"这样的请求时，模型可以同时调用两个城市的天气 API，然后基于返回结果给出合理建议，提高处理复杂任务的效率。\\n多路召回（Multi-path Retrieval）\\n多路召回是通过多种检索方法并行获取信息的策略。例如，系统可以同时使用关键词搜索、语义匹配和知识图谱查询，然后合并筛选结果，提高信息获取的覆盖面和准确性，特别适合处理复杂或模糊的用户查询。\\nP\\n父子分段模式（Parent-Child Chunking）\\n父子分段模式是一种高级文本分割策略，创建两层级的内容块：父区块保留完整上下文，子区块提供精确匹配点。系统先通过子区块确定相关内容位置，再获取对应父区块以提供完整背景，同时兼顾检索精度和上下文完整性，适合处理复杂文档如研究论文或技术手册。\\n存在惩罚（Presence Penalty）\\n存在惩罚是防止语言模型重复内容的参数设置。它通过降低已出现词汇的生成概率，鼓励模型探索新的表达方式。参数值越高，模型越不倾向于重复之前生成的内容，有助于避免 AI 回答中常见的循环论证或重复叙述问题。\\n预定义模型（Predefined Model）\\n预定义模型是由 AI 厂商训练并提供的现成模型，用户可以直接调用而无需自行训练。这些闭源模型（如 GPT-4、Claude 等）通常经过大规模训练和优化，能力强大且易于使用，适合快速开发应用或缺乏自主训练资源的团队。\\n提示词（Prompt）\\n提示词是引导 AI 模型生成特定响应的输入文本。精心设计的提示词能显著提高输出质量，包括明确指令、提供示例、设定格式要求等元素。例如，不同的提示词可以引导同一模型生成学术文章、创意故事或技术分析，是影响 AI 输出的最关键因素之一。\\nQ\\n问答模式（Q\\\\&A Mode）\\n问答模式是一种特殊索引策略，为文档内容自动生成问答对，实现\"问题到问题\"的匹配。当用户提问时，系统会寻找语义相似的预生成问题，然后返回对应答案。这种模式特别适合 FAQ 内容或结构化知识点，能提供更精准的问答体验。\\nR\\n检索增强生成（RAG, Retrieval-Augmented Generation）\\n检索增强生成是结合外部知识检索和语言生成的技术架构。系统首先从知识库检索与用户问题相关的信息，然后将这些信息作为上下文提供给语言模型，生成有依据、准确的回答。RAG 克服了语言模型知识有限和幻觉问题，特别适合需要最新或专业知识的应用场景。\\n推理与行动（ReAct, Reasoning and Acting）\\n推理与行动是一种 AI 代理框架，使模型能够交替进行思考和执行操作。在解决问题过程中，模型先分析当前状态，制定计划，然后调用合适工具（如搜索引擎、计算器），根据工具返回结果进行下一步思考，形成思考-行动-思考的循环，直到解决问题，适合处理需要多步骤和外部工具的复杂任务。\\n重排序（ReRank）\\n重排序是对初步检索结果进行二次排序的技术，提高最终结果的相关性。例如，系统可能先通过高效算法快速检索出大量候选内容，然后使用更复杂但精准的模型对这些结果重新评分排序，将最相关的内容置前，平衡了检索效率和结果质量。\\n重新排序模型（Rerank Model）\\n重新排序模型专门用于评估检索结果与查询的相关性并重新排序。与初步检索不同，这类模型通常采用更复杂的算法，考虑更多语义因素，能更精确地判断内容与用户意图的匹配度。例如，Cohere Rerank 和 BGE Reranker 等模型可显著提升搜索和推荐系统的结果质量。\\n响应格式（Response Format）\\n响应格式指定模型输出的结构类型，如纯文本、JSON 或 HTML。设置特定的响应格式可以使 AI 输出更容易被程序处理或集成到其他系统。例如，要求模型以 JSON 格式回答可以确保输出具有一致的结构，便于前端应用直接解析和展示。\\n反向调用（Reverse Calling）\\n反向调用是插件与平台交互的双向机制，允许插件主动调用平台功能。在 Dify 中，这意味着第三方插件不仅能被 AI 调用，还能反过来使用 Dify 的核心功能，如触发工作流或调用其他插件，极大增强了系统的扩展性和灵活性。\\n召回测试（Retrieval Test）\\n召回测试是验证知识库检索效果的功能，开发者可以模拟用户查询并评估系统返回结果。这种测试帮助开发者了解系统的检索能力边界，发现并修复潜在问题，如漏检、误检或相关度不佳的情况，是优化 RAG 系统不可或缺的工具。\\nS\\n分数阈值（Score Threshold）\\n分数阈值是过滤检索结果的相似度门槛，只有评分超过设定值的内容才会被返回。设置合理的阈值可以避免无关信息干扰模型生成，提高回答的精确性。例如，如果阈值设为 0.8（满分 1.0），则只有高度相关的内容会被采用，但可能导致信息不全；降低阈值则会纳入更多内容但可能引入噪音。\\n软件开发工具包（SDK, Software Development Kit）\\n软件开发工具包是一组用于开发特定平台或服务应用程序的工具集合。在 Dify 中，SDK 提供了各种编程语言的接口库，使开发者能够方便地集成和调用 Dify 的功能，包括对话管理、知识库操作、应用配置等。通过使用 SDK，开发者可以在自己的应用中快速实现 AI 能力，而无需深入了解底层 API 细节。\\n语义检索（Semantic Search）\\n语义检索基于理解和匹配文本意义而非简单关键词匹配的检索方法。它利用向量嵌入技术将文本转换为数学表示，然后计算查询与文档的语义相似度。这种方法能够找到表达方式不同但含义相近的内容，理解同义词和上下文关系，甚至支持跨语言检索，特别适合复杂或自然语言形式的查询。\\n会话变量（Session Variables）\\n会话变量是存储多轮对话上下文信息的机制，使 AI 能维持连贯交互。例如，系统可以记住用户的偏好（如\"简洁回答\"）、身份信息或交互历史状态，避免重复询问，提供个性化体验。在 Dify 中，开发者可以定义和管理这些变量，建立真正记住用户的\"有记忆\"应用。\\n语音转文字（STT, Speech-to-Text）\\n语音转文字技术将用户的语音输入转换为文本数据。这项技术让用户可以通过说话而非打字与 AI 系统交互，提高了交互的自然性和便捷性，特别适合移动设备、驾驶场景或无障碍应用，是语音助手和实时转录等应用的基础。\\n流式工具调用（Stream-tool-call）\\n流式工具调用是一种实时处理模式，允许 AI 系统在生成响应的同时调用外部工具，而不必等待完整回答生成后再处理。这种方式大大提高了处理复杂任务的响应速度，让用户体验更加流畅，适合需要多次工具调用的交互场景。\\n流式结果返回（Streaming Response）\\n流式结果返回是一种实时响应机制，AI 系统边生成内容边返回给用户，而不是等所有内容生成完毕再一次性展示。这种方式显著改善用户等待体验，特别是对于长回答，用户可以立即看到部分内容并开始阅读，提供更自然的交互感受，类似于人类对话中的即时反馈。\\nT\\n温度（Temperature）\\n温度是控制语言模型输出随机性的参数，通常在 0-1 之间。温度越低（接近 0），模型输出越确定和保守，倾向于高概率词汇，适合事实性回答；温度越高（接近 1），输出越多样和创造性，适合创意写作。例如，天气预报可能使用 0.1 的低温度，而故事创作可能使用 0.8 的高温度。\\n文本嵌入（Text Embedding）\\n文本嵌入是将文本转换为数值向量的过程，使 AI 系统能够理解和处理语言。这些向量捕捉了词汇和句子的语义特征，使计算机可以测量文本间的相似度、聚类相关内容或检索匹配信息。不同的嵌入模型（如 OpenAI 的 text-embedding-ada-002 或 Cohere 的 embed-multilingual）针对不同语言和应用场景进行了优化。\\n工具调用（Tool Calling）\\n工具调用是 AI 系统识别并使用外部功能的能力，极大扩展了模型的能力边界。例如，语言模型本身不能访问实时数据，但通过调用天气 API，它可以提供当前天气信息；通过调用数据库查询工具，它可以获取最新产品库存；通过调用计算器，它可以执行复杂计算，这使 AI 能够解决超出其训练数据范围的问题。\\nTopK\\nTopK 是控制检索返回结果数量的参数，指定保留相似度最高的前 K 个文本片段。合理设置 TopK 值对 RAG 系统性能至关重要：值太小可能丢失关键信息，值太大则可能引入噪音并增加语言模型处理负担。例如，简单问题可能只需 TopK=3，而复杂问题可能需要 TopK=10 以获取足够背景。\\n核采样（TopP, Nucleus Sampling）\\n核采样是一种文本生成控制方法，只从累积概率达到阈值 P 的最可能词汇中选择下一个词。与固定选择最高概率词或完全随机不同，TopP 在确定性和创造性间取得平衡。例如，TopP=0.9 意味着模型只考虑概率和占 90% 的词汇，忽略低概率选项，既避免了完全可预测的输出，又不会生成过于随机的内容。\\n思维树（ToT, Tree of Thought）\\n思维树是一种探索多个推理路径的思考方法，允许模型从不同角度分析问题。类似于人类的\"如果...那么...\"思考模式，思维树让模型生成多个可能的思考分支，评估每个分支的可行性，然后选择最优路径继续，特别适合解决需要试错或考虑多种可能性的复杂问题。\\n文本转语音（TTS, Text-to-Speech）\\n文本转语音是将书面文本转换为自然语音的技术，使 AI 系统能以语音方式与用户交流。现代 TTS 系统能生成接近人类的自然语音，支持多种语言、音色和情感表达，广泛应用于有声读物、导航系统、语音助手和无障碍服务，为不同场景和用户提供更自然的交互体验。\\nV\\n向量数据库（Vector Database）\\n向量数据库是专门存储和搜索向量嵌入的数据库系统，是高效语义检索的基础设施。与传统数据库不同，向量数据库针对高维向量相似度搜索进行了优化，能快速从数百万文档中找出语义相近的内容。常见的向量数据库包括 Pinecone、Milvus、Qdrant 等，它们在 RAG 系统、推荐引擎和内容分析中发挥关键作用。\\n向量检索（Vector Retrieval）\\n向量检索是基于文本向量嵌入相似度的搜索方法，是语义搜索的技术核心。系统首先将用户查询转换为向量，然后在预先计算的文档向量中查找最相似的内容。这种方法能够捕捉深层语义关系，找到表达不同但意思相近的内容，克服了关键词搜索的局限，特别适合处理自然语言查询和概念性问题。\\n视觉能力（Vision）\\n视觉能力是多模态 LLM 理解和处理图像的功能，允许模型分析用户上传的图片并结合文本生成回答。例如，用户可以上传产品照片询问使用方法，上传菜单照片请求翻译，或上传图表要求分析数据趋势。这种能力大大拓展了 AI 应用场景，使交互更加直观和多样化。\\nW\\n工作流（Workflow）\\n工作流是一种任务编排方式，将复杂 AI 应用拆分为多个独立节点并按特定顺序执行。在 Dify 平台中，开发者可以可视化设计工作流，组合多个处理步骤（如用户输入处理、知识检索、多模型协作、条件分支），构建能处理复杂业务逻辑的 AI 应用，使应用开发既灵活又直观。')",
         "('type', 'Document')"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(id, None)</td>\n",
       "      <td>(metadata, {'source': 'corpus/LLM术语表.md'})</td>\n",
       "      <td>(page_content, A\\n智能代理（Agent）\\n智能代理是一种自主 AI 系统...</td>\n",
       "      <td>(type, Document)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0                                           1  \\\n",
       "0  (id, None)  (metadata, {'source': 'corpus/LLM术语表.md'})   \n",
       "\n",
       "                                                   2                 3  \n",
       "0  (page_content, A\\n智能代理（Agent）\\n智能代理是一种自主 AI 系统...  (type, Document)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(docs)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 初始化支持中文的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_settings import BaseSettings, SettingsConfigDict\n",
    "from pydantic import SecretStr\n",
    "\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    model_config = SettingsConfigDict(\n",
    "        env_file=\".env\", env_file_encoding=\"utf-8\", extra=\"ignore\", case_sensitive=False\n",
    "    )\n",
    "    openai_api_key: SecretStr\n",
    "    openai_base_url: str\n",
    "    openai_model: str  # 评估用LLM\n",
    "    openai_embedding_model: str  # 嵌入模型\n",
    "    temperature: int = 0  # 固定随机种子，确保评估结果稳定\n",
    "\n",
    "\n",
    "config = Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'openai_api_key': SecretStr('**********'),\n",
       " 'openai_base_url': 'http://192.168.3.2:11434/v1',\n",
       " 'openai_model': 'qwen3',\n",
       " 'openai_embedding_model': 'bge-m3',\n",
       " 'temperature': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=config.openai_base_url,\n",
    "    api_key=config.openai_api_key,\n",
    "    model=config.openai_model,\n",
    "    temperature=config.temperature,\n",
    ")\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=config.openai_embedding_model,\n",
    "    base_url=config.openai_base_url,\n",
    "    api_key=config.openai_api_key,\n",
    ")\n",
    "\n",
    "generator_llm = LangchainLLMWrapper(llm)\n",
    "generator_embedding = LangchainEmbeddingsWrapper(embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 设置中文角色和转换工具\n",
    "### 1.3.1 定义中文场景的用户角色"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.persona import Persona\n",
    "\n",
    "personas = [\n",
    "    Persona(\n",
    "        name=\"中文四大名著学习者\",\n",
    "        role_description=\"  一位对中国古典文学四大名著（《红楼梦》、《三国演义》、《水浒传》、《西游记》）感兴趣的学习者，希望通过查询深入了解这些作品的内容、背景和文学价值。\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 配置中文适用的转换工具（如标题分割、实体提取）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "from ragas.testset.transforms.extractors.llm_based import NERExtractor\n",
    "from ragas.testset.transforms.splitters import HeadlineSplitter\n",
    "\n",
    "transforms = [HeadlineSplitter(), NERExtractor(llm=generator_llm)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 初始化测试生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(\n",
    "    llm=generator_llm,\n",
    "    embedding_model=generator_embedding,\n",
    "    persona_list=personas,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 加载查询类型并适配中文\n",
    "### 1.5.1 定义单跳查询生成器并适配中文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.synthesizers.single_hop.specific import (\n",
    "    SingleHopSpecificQuerySynthesizer,\n",
    ")\n",
    "\n",
    "\n",
    "distribution = [\n",
    "    (SingleHopSpecificQuerySynthesizer(llm=generator_llm), 1.0),\n",
    "]\n",
    "# 将查询提示词适配为中文\n",
    "for query, _ in distribution:\n",
    "    prompts = await query.adapt_prompts(\n",
    "        \"chinese\", llm=generator_llm\n",
    "    )  # 指定目标语言为中文\n",
    "    query.set_prompts(**prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 生成中文测试集\n",
    "### 1.6.1 基于中文文档生成查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0fd3a12df1453895951a418b015d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlineSplitter:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to apply transformation: 'headlines' property not found in this node\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c015b48d9854c4489eeae4f4c812af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying NERExtractor:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m dataset = \u001b[43mgenerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_with_langchain_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtestset_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 生成3条中文查询\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_distribution\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdistribution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/ragas-code/.venv/lib/python3.12/site-packages/ragas/testset/synthesizers/generate.py:185\u001b[39m, in \u001b[36mTestsetGenerator.generate_with_langchain_docs\u001b[39m\u001b[34m(self, documents, testset_size, transforms, transforms_llm, transforms_embedding_model, query_distribution, run_config, callbacks, with_debugging_logs, raise_exceptions)\u001b[39m\n\u001b[32m    182\u001b[39m kg = KnowledgeGraph(nodes=nodes)\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# apply transforms and update the knowledge graph\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m \u001b[43mapply_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[38;5;28mself\u001b[39m.knowledge_graph = kg\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.generate(\n\u001b[32m    189\u001b[39m     testset_size=testset_size,\n\u001b[32m    190\u001b[39m     query_distribution=query_distribution,\n\u001b[32m   (...)\u001b[39m\u001b[32m    194\u001b[39m     raise_exceptions=raise_exceptions,\n\u001b[32m    195\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/ragas-code/.venv/lib/python3.12/site-packages/ragas/testset/transforms/engine.py:106\u001b[39m, in \u001b[36mapply_transforms\u001b[39m\u001b[34m(kg, transforms, run_config, callbacks)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transforms, t.List):\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m transforms:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m         \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_coroutines\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_execution_plan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m                \u001b[49m\u001b[43mget_desc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# if Parallel, collect inside it and run it all\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transforms, Parallel):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/ragas-code/.venv/lib/python3.12/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/ragas-code/.venv/lib/python3.12/site-packages/nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/ragas-code/.venv/lib/python3.12/site-packages/nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.10-macos-aarch64-none/lib/python3.12/selectors.py:566\u001b[39m, in \u001b[36mKqueueSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    564\u001b[39m ready = []\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m     kev_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "dataset = generator.generate_with_langchain_docs(\n",
    "    docs,\n",
    "    testset_size=3,  # 生成3条中文查询\n",
    "    transforms=transforms,\n",
    "    query_distribution=distribution,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.2 转换为评估数据集并查看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "eval_dataset = dataset.to_evaluation_dataset()\n",
    "# 打印第一条中文查询和参考文本\n",
    "print(\"用户查询:\", eval_dataset[0].user_input)\n",
    "print(\"参考回答:\", eval_dataset[0].reference)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
